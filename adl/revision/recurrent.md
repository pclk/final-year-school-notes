# deck: adl recurrent
test

What is the defining property of Sequential data?
If you shuffle it, you lose core information.
---
Sequential data: Order matters. Think time series, language, signals, processes unfolding step-by-step.  If you shuffle it, you lose crucial information.
Non-sequential data: Order doesn't fundamentally matter for the core information. Think collections of features, categories, single values. Shuffling might lose context, but the main information is still there.

The text classification workflow for deep learning is 1) -Text processing-, 2) -Word Embedding Layer-, 3) -Recurrent Layer-, 4) -Fully Connected Layer-, 5) -Classification-.
> Memory: TWRFC - Teachers Work Really For Children
> Text processing: stopword removal, tokenization...
> Word embedding layer: Convert tokens to vector representations called word embeddings. Example: Word2Vec.
> Recurrent layer: Process sequential data, understand order of words, capture contextual info. Produces hidden states.
> Fully Connected Layer: Known as Dense layer / Feedforward layer. Transforms the hidden states into classification task.
> Classification: Output layer.

What is the recursive part of an RNN?
Reusing same function for each element in input sequence.
---
The RNN recursively calls the function that generates the hidden state until the end of the input sequence is reached.

The inputs of a function in a RNN is -current input element- and -hidden state-.

The outputs of a function in a RNN is -current output element- and -hidden state-.

What is a simple way to think of hidden states generated by RNN?
Memory of past information.

What is the initial hidden state in a RNN?
It's often a vector of zeros.

Bidirectional RNNs does a -forward pass-, a -backward pass-, then 1.-combines- their 1.-hidden states-.
> Memory: Imagine a line that passes left to right a series of balls, then reverts back right to left.
> This allows context from both directions, leading to improved accuracy.

Depth in RNNs do not just refer to -multiple layers-, but also in terms of -timesteps-.
> RNNs are already deep in the temporal dimension, or sequence length. Think of an RNN processing a sentence word by word, unrolling many times, and making it deep in that sense.

Multi-layer RNNs work by receiving the -hidden states- from the previous RNN layer.
> First layer receives original input sequence and generates hidden states.
> Second layer receives hidden states and generates its own hidden states.
> Subsequently so.

What is the relationship between hidden state and output?
Output is derived from hidden state
---
It's a transformation of the hidden state to produce something relevant to the task at hand.  Often, a linear layer (fully connected layer) is applied to the hidden state to generate the output.

Why do traditional RNNs struggle with long input sequences?
Gradient signal is weaker the earlier it is.
---
Because of the mathematical operations involved in backpropagation, especially if the network uses certain types of activation functions (like sigmoid or tanh, which were common in older RNNs).  These functions can squash the gradient values, making them smaller than 1.  When you multiply many numbers that are less than 1 together, the result gets smaller and smaller, very quickly approaching zero.

What kind of bias do traditional RNNs with long input sequences suffer?
Short-term memory bias
---
Because the gradients from recent timesteps are stronger (they haven't vanished as much), the RNN becomes much better at learning short-term dependencies â€“ things that are close together in the sequence.  It becomes overly focused on the "now" and forgets the "past".

Why does Long Short-Term Memory (LSTM) solve the vanishing gradient problem?
The network can update it's long term memory (cell state) 
---
A Notebook (Cell State): This is your long-term memory storage. Unlike the tiny sticky note in vanilla RNNs, this notebook preserves information for a long time.

What are the three Gates of Long Short-Term Memory (LSTM)?
Input, Forget and Output Gate.
---
Input Gate: This assistant decides "What new information should we write down?" They evaluate new information and decide if it's important enough to add to the notebook.
Forget Gate: This assistant decides "What old information should we erase?" They review the notebook and determine which notes are no longer relevant.
Output Gate: - This assistant decides "What information should we focus on right now?" They determine what parts of our notes are relevant for the current moment.

Why can RNN process any length of input?
Doesn't need to be redesigned
---
Just like how you can read any book regardless of its length - whether it's a short story or a lengthy novel

Why can RNN, in theory, use information from many steps back?
Cell and hidden state
---
While reading, you can remember plot points from previous chapters to understand the current scene

Why doesn't RNN's model size increase for longer input?
Can apply same weights for every timestep.
---
Think of it like having a single set of reading skills that you apply repeatedly

Why is recurrent computation slow?
Like reading a book, you must go through it sequentially
You can't truly read Chapter 1 and Chapter 5 simultaneously

Why is it, in practice, difficult to access information from many steps back as an RNN?
Vanishing gradient problem
---
Similar to trying to remember exact details from Chapter 1 when you're in Chapter 20
