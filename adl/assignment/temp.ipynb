{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "PmegzSqhZh"
      },
      "source": [
        "# Assignment 2024 S2\n",
        "## Part 2: Structured Data - Direct vs Indirect Training Data\n",
        "\n",
        "Citation: Some code has been generated with the help of Claude 3.5 Sonnet by Anthropic, and some decisions and further clarifications were made with gemini-2.0-flash-thinking-exp-01-21\n",
        "\n",
        "drugName (categorical): name of drug\n",
        "\n",
        "condition (categorical): name of condition\n",
        "\n",
        "review (text): patient review\n",
        "\n",
        "rating (numerical): 10 star patient rating\n",
        "\n",
        "date (date): date of review entry\n",
        "\n",
        "usefulCount (numerical): number of users who found review useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "5ERatCyjWG"
      },
      "source": [
        "# Standard library imports\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Data handling & visualization\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "NRPCotjWkP"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "XMoe5OcosM"
      },
      "source": [
        "INPUT_TRAIN = \"drug_review_train.csv\"\n",
        "INPUT_TEST = \"drug_review_test.csv\"\n",
        "\n",
        "INPUT_TRAIN = \"drug_review_train.csv\"\n",
        "INPUT_TEST = \"drug_review_test.csv\"\n",
        "\n",
        "df = pd.read_csv(INPUT_TRAIN)\n",
        "df_test = pd.read_csv(INPUT_TEST)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "pUXvOro0RW"
      },
      "source": [
        "# # --- Data Profiling ---\n",
        "# print(\"Profiling train data...\")\n",
        "# profile_train = ProfileReport(\n",
        "#     df,\n",
        "#     title=\"Drug Review Train Data Profiling Report\",\n",
        "#     explorative=True,\n",
        "# )\n",
        "# profile_train.to_file(\"drug_review_train_profiling.html\")\n",
        "\n",
        "# print(\"Profiling test data...\")\n",
        "# profile_test = ProfileReport(\n",
        "#     df_test,\n",
        "#     title=\"Drug Review Test Data Profiling Report\",\n",
        "#     explorative=True,\n",
        "# )\n",
        "# profile_test.to_file(\"drug_review_test_profiling.html\")\n",
        "# print(\"Profiling complete. Reports saved as HTML files.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "WtG5isDuO7"
      },
      "source": [
        "# Create a DataFrame with column descriptions\n",
        "column_info = {\n",
        "    \"Column Name\": [\"drugName\", \"condition\", \"review\", \"rating\", \"date\", \"usefulCount\"],\n",
        "    \"Data Type\": [\n",
        "        \"categorical\",\n",
        "        \"categorical\",\n",
        "        \"text\",\n",
        "        \"numerical\",\n",
        "        \"date\",\n",
        "        \"numerical\",\n",
        "    ],\n",
        "    \"Description\": [\n",
        "        \"Name of the drug\",\n",
        "        \"Name of the medical condition\",\n",
        "        \"Patient review text\",\n",
        "        \"10-star patient rating\",\n",
        "        \"Date of review entry\",\n",
        "        \"Number of users who found review useful\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "column_df = pd.DataFrame(column_info)\n",
        "column_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "1DpQGj5q3J"
      },
      "source": [
        "# Analyze usefulCount zeros\n",
        "df = pd.read_csv(INPUT_TRAIN)\n",
        "zero_useful_count = (df[\"usefulCount\"] == 0).sum()\n",
        "total_reviews = len(df)\n",
        "zero_percentage = (zero_useful_count / total_reviews) * 100\n",
        "\n",
        "print(f\"\\n--- UsefulCount Analysis ---\")\n",
        "print(f\"Total reviews: {total_reviews:,}\")\n",
        "print(f\"Reviews with zero useful votes: {zero_useful_count:,}\")\n",
        "print(f\"Percentage of zero useful votes: {zero_percentage:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "5h8cA9wq2C"
      },
      "source": [
        "### Analysis of Zero UsefulCount Impact on Sentiment:\n",
        "\n",
        "1. Silent Majority Phenomenon:\n",
        "   - The high percentage of zero useful votes suggests a classic \"lurker\" behavior in online communities\n",
        "   - Most users read but don't interact, creating a participation inequality\n",
        "   - This means our sentiment analysis might be biased towards more \"engaging\" content\n",
        "\n",
        "2. Sentiment Validation Gap:\n",
        "   - Reviews with zero useful votes lack community validation\n",
        "   - We can't assume these reviews are less valuable - they might be newer or simply not seen by many users\n",
        "   - This creates a potential temporal bias in our sentiment understanding\n",
        "\n",
        "3. Engagement vs. Sentiment Relationship:\n",
        "   - Higher useful counts might indicate more polarizing content rather than more accurate sentiment\n",
        "   - Extreme opinions (very positive or very negative) tend to attract more engagement\n",
        "   - This suggests we should be cautious about weighing sentiment by useful counts\n",
        "\n",
        "4. Data Quality Implications:\n",
        "   - Zero useful counts might indicate:\n",
        "     a) Fresh reviews that haven't had time to accumulate votes\n",
        "     b) Reviews that didn't reach many readers\n",
        "     c) Reviews that readers found neither particularly helpful nor controversial\n",
        "   - This impacts how we should approach sentiment weighting in our analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "jQErnVDvCH"
      },
      "source": [
        "print(f\"\\n--- UsefulCount Analysis ---\")\n",
        "print(f\"Total reviews: {total_reviews:,}\")\n",
        "print(f\"Reviews with zero useful votes: {zero_useful_count:,}\")\n",
        "print(f\"Percentage of zero useful votes: {zero_percentage:.2f}%\")\n",
        "\n",
        "# Create correlation matrix\n",
        "print(\"\\n--- Correlation Matrix Analysis ---\")\n",
        "# Select only numerical columns\n",
        "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.rcParams.update({\"font.size\": 14})\n",
        "plt.figure(figsize=(10, 8), dpi=400)\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True,  # Show correlation values\n",
        "    fmt=\".3f\",\n",
        ")  # Format correlation values to 2 decimal places\n",
        "plt.title(\"Correlation Matrix of Numerical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "uQbS7qTWhr"
      },
      "source": [
        "### Key Correlation Findings:\n",
        "- Correlation between rating and usefulCount: 0.243\n",
        "- Most other numerical features show weak or no correlation\n",
        "- This suggests that higher rated reviews tend to be found slightly more useful by readers, or vice versa\n",
        "\n",
        "### Should we include usefulCount?\n",
        "usefulCount is a similar metric to ratings, and its logical to think that in situations you don't have ratings, you probably wouldn't have the usefulCount too. \n",
        "\n",
        "Some cases of situations where you don't have ratings are if you're predicting how positive a review is. Usually, you would only have the review and the patientid only, while the review_length can be determined from review. Thus, it may not be fair to include usefulCount in our machine learning, especially if we're focusing on text sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "eziwylDdbF"
      },
      "source": [
        "# Create histogram of ratings with percentage labels\n",
        "plt.figure(figsize=(12, 6), dpi=400)\n",
        "\n",
        "\n",
        "# Calculate histogram data\n",
        "counts, bins, _ = plt.hist(df[\"rating\"], bins=10, edgecolor=\"black\")\n",
        "total = len(df[\"rating\"])\n",
        "\n",
        "for i in range(len(counts)):\n",
        "    percentage = (counts[i] / total) * 100\n",
        "    plt.text(bins[i], counts[i], f\"{percentage:.1f}%\", va=\"bottom\")\n",
        "\n",
        "plt.title(\"Distribution of Drug Ratings\")\n",
        "plt.xlabel(\"Rating (0-10)\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "PF2e2VX7Uv"
      },
      "source": [
        "The question is how can we split this into positive and negative sentiment?\n",
        "\n",
        "1. **Highly Imbalanced Distribution**:\n",
        "   - Ratings 9 and 10 dominate (48.4% combined)\n",
        "   - Rating 10 alone is 30.9% of all reviews\n",
        "   - Ratings 3, 4, and 6 are severely underrepresented (each around 3-4%)\n",
        "   - This creates a significant class imbalance problem\n",
        "\n",
        "2. **Bimodal Distribution**:\n",
        "   - There are two peaks: Rating 1 (12.9%) and Ratings 9-10 (48.4%)\n",
        "   - This suggests strong polarization in reviews\n",
        "   - Middle ratings (3-6) are less common\n",
        "   - This validates our earlier decision to split into two classes (1-6 vs 7-10)\n",
        "\n",
        "3. **Machine Learning Implications**:\n",
        "\n",
        "   a) **Class Imbalance Solutions Needed**:\n",
        "   - Consider using class weights\n",
        "   - Implement oversampling (SMOTE) for minority classes\n",
        "   - Use undersampling for majority classes\n",
        "   - Or combine both (SMOTEENN, SMOTETomek)\n",
        "\n",
        "   b) **Evaluation Metrics**:\n",
        "   - Accuracy alone would be misleading\n",
        "   - Need to focus on:\n",
        "     * F1-score\n",
        "     * Precision and Recall\n",
        "     * ROC-AUC\n",
        "     * Confusion matrix analysis\n",
        "\n",
        "   c) **Model Selection**:\n",
        "   - Choose algorithms that handle imbalanced data well\n",
        "   - Consider ensemble methods\n",
        "   - Use stratification in train/test splits\n",
        "\n",
        "4. **Neural Network Considerations**:\n",
        "   - The imbalanced distribution affects deep learning models differently than traditional ML:\n",
        "     * Deep learning models often need MORE data per class for effective learning\n",
        "     * The severe underrepresentation of ratings 3-6 (each ~3-4%) is particularly problematic\n",
        "     * The dominance of rating 10 (30.9%) could cause model bias\n",
        "\n",
        "5. **Deep Learning Specific Solutions**:\n",
        "\n",
        "   a) **Data Augmentation**:\n",
        "   - For text data, we can use:\n",
        "     * Back-translation\n",
        "     * Synonym replacement\n",
        "     * Text generation using LLMs\n",
        "     * EDA (Easy Data Augmentation) techniques\n",
        "   - These help increase samples for underrepresented ratings\n",
        "\n",
        "   b) **Loss Functions**:\n",
        "   - Use specialized loss functions:\n",
        "     * Weighted Cross-Entropy Loss\n",
        "     * Focal Loss (reduces impact of easy, common samples)\n",
        "     * Class-Balanced Loss\n",
        "   - These help handle class imbalance during training\n",
        "\n",
        "   c) **Architecture Choices**:\n",
        "   - Consider:\n",
        "     * Pre-trained language models (BERT, RoBERTa)\n",
        "     * Multi-task learning approaches\n",
        "     * Attention mechanisms to focus on important parts of reviews\n",
        "   - These help leverage the bimodal nature of the distribution\n",
        "\n",
        "6. **Training Strategies**:\n",
        "   - Implement:\n",
        "     * Gradient accumulation\n",
        "     * Progressive resizing\n",
        "     * Curriculum learning (start with balanced subsets)\n",
        "   - Use dynamic batch sampling:\n",
        "     * Over-sample minority classes within batches\n",
        "     * Ensure each batch sees all rating classes\n",
        "\n",
        "7. **Validation Considerations**:\n",
        "   - Use:\n",
        "     * Stratified k-fold cross-validation\n",
        "     * Balanced validation sets\n",
        "     * Multiple evaluation metrics\n",
        "   - Monitor for:\n",
        "     * Overfitting on majority classes\n",
        "     * Underfitting on minority classes\n",
        "     * Class-wise performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "cSLy8xXXnb"
      },
      "source": [
        "def wrap_text(text, width=80, indent=4):\n",
        "    \"\"\"\n",
        "    Custom function to wrap text with indentation\n",
        "    Args:\n",
        "        text (str): The text to wrap\n",
        "        width (int): Maximum width of each line\n",
        "        indent (int): Number of spaces for indentation\n",
        "    Returns:\n",
        "        str: Wrapped and indented text\n",
        "    \"\"\"\n",
        "    # Split text into words\n",
        "    words = text.split()\n",
        "    # Initialize variables\n",
        "    lines = []\n",
        "    current_line = \" \" * indent  # Start with indentation\n",
        "    current_width = indent\n",
        "\n",
        "    for word in words:\n",
        "        # Calculate width if we add this word\n",
        "        if current_width + len(word) + 1 <= width:\n",
        "            # Add word with a space\n",
        "            if current_width > indent:  # If not the first word in line\n",
        "                current_line += \" \"\n",
        "                current_width += 1\n",
        "            current_line += word\n",
        "            current_width += len(word)\n",
        "        else:\n",
        "            # Line is full, start a new line\n",
        "            lines.append(current_line)\n",
        "            current_line = \" \" * indent + word\n",
        "            current_width = indent + len(word)\n",
        "\n",
        "    # Add the last line\n",
        "    if current_line:\n",
        "        lines.append(current_line)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# Sample reviews from each rating category\n",
        "N_REVIEWS = 10\n",
        "print(\"\\n=== Sample Reviews by Rating ===\")\n",
        "for rating in sorted(df[\"rating\"].unique()):\n",
        "    print(f\"\\nRating {rating:.1f} - {N_REVIEWS} Sample Reviews:\")\n",
        "    print(\"-\" * 80)\n",
        "    sample_reviews = df[df[\"rating\"] == rating].sample(\n",
        "        n=min(N_REVIEWS, len(df[df[\"rating\"] == rating]))\n",
        "    )\n",
        "    # Display in a more readable format\n",
        "    for idx, row in sample_reviews.iterrows():\n",
        "        print(f\"Drug: {row['drugName']}\")\n",
        "        print(f\"Condition: {row['condition']}\")\n",
        "        print(f\"UsefulCount: {row['usefulCount']}\")\n",
        "        print(\"Review:\")\n",
        "        # Use our custom wrap_text function\n",
        "        wrapped_review = wrap_text(row[\"review\"], width=80, indent=4)\n",
        "        print(wrapped_review)\n",
        "        print(\"-\" * 80)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "abNrMWMlZZ"
      },
      "source": [
        "Summary:\n",
        "\n",
        "The reviews highlight a wide range of experiences, from severe negative side effects and dissatisfaction to significant relief and positive outcomes. Many reviews, especially at the lower ratings, focus on negative side effects such as nausea, weight gain, bleeding, mood changes, and digestive issues. Higher-rated reviews often acknowledge some initial side effects but emphasize the drug's effectiveness in treating the condition. Some medium-rated reviews acknowledge postive side effects of the drug, but not effective overall.\n",
        "\n",
        "Sentiment Threshold:\n",
        "\n",
        "Based on the provided samples, the sentiment threshold appears to be around a rating of 7.0.\n",
        "\n",
        "Sentiment Threshold Analysis:\n",
        "\n",
        "Looking closely at the reviews within each rating level, and paying attention to how the language and described experiences change, here's a refined breakdown and the apparent threshold:\n",
        "\n",
        "1.0: Almost universally extremely negative. Users describe severe, debilitating side effects, complete lack of effectiveness, and often dangerous reactions. Words like \"horrible,\" \"awful,\" \"die,\" \"severe pain,\" and descriptions of emergency room visits are common. These are clearly negative experiences.\n",
        "\n",
        "2.0: Still overwhelmingly negative. The language is similar to the 1.0 reviews, focusing on significant side effects, lack of efficacy, and regret. There's a sense of frustration and disappointment. Some reviews mention stopping the medication due to the negative experience.\n",
        "\n",
        "3.0: Predominantly negative, but with hints of mixed experiences. While many reviews still detail significant side effects and problems, some acknowledge potential benefits or that the drug might work for others, even if it didn't work for them. There's more ambivalence here, but the overall tone leans negative. We see phrases like \"takes some getting used to,\" \"overwhelming,\" \"not worth it,\" and descriptions of weight gain, mood changes, and other undesirable effects.\n",
        "\n",
        "4.0: A definite mix of negative and slightly more neutral experiences, but still leaning negative overall. Users often describe a trade-off: the medication might help with the condition to some extent, but the side effects are significant and disruptive. There's a sense of weighing pros and cons, and often the cons are still winning. We see mentions of both positive effects (e.g., \"worked for a few years,\" \"pain was less\") and negative ones (\"wasn't pleasant,\" \"side effects such as lack of concentration,\" \"gaining weight\").\n",
        "\n",
        "5.0: Truly mixed, and the most difficult to categorize neatly. These reviews represent a clear \"tipping point.\" Some users report positive effects on the condition, but significant side effects often counterbalance those benefits. Other users report minimal benefits and persistent problems. There's a strong sense of individual variability and uncertainty. The language is less intensely negative than lower ratings, but still expresses concern and dissatisfaction. Key phrases: \"hit or miss,\" \"side effects were slim in the first couple of months but soon after...,\" \"worked really good for that [one symptom]... [but had other significant negative effects].\"\n",
        "\n",
        "6.0: Similar to 5.0, a mix of positive and negative, but with a slight shift towards acknowledging benefits, even with ongoing issues. The reviews often describe a situation where the drug helps, but the side effects are still a significant factor, leading to a less-than-ideal experience. There's a sense of compromise and ongoing evaluation. We see phrases like \"love/hate relationship,\" \"better than [previous medication],\" \"side effects improved,\" and \"debating whether i should stop.\"\n",
        "\n",
        "7.0: This is where the sentiment generally shifts to positive, but with caveats. Users often describe a \"learning curve\" or initial side effects that diminished over time. The reviews tend to emphasize the drug's effectiveness in managing the condition, while still acknowledging some lingering drawbacks or individual concerns. There's more optimism and a sense of finding a workable solution. Key phrases: \"helped me a lot,\" \"pros definitely outweigh the cons,\" \"worked great for the first 2 years, but...,\" \"worked really well [but had side effects].\"\n",
        "\n",
        "8.0: More consistently positive. Users report good results and often express satisfaction with the medication. Side effects are either minimal, manageable, or considered worth enduring for the benefits. There's a sense of finding a good balance and a willingness to continue treatment. Phrases like \"worked miracles,\" \"feel so much better,\" \"life saver,\" and \"good cushion for my knees\" appear.\n",
        "\n",
        "9.0: Strongly positive, with users often describing significant improvements and a high level of satisfaction. Side effects are mentioned less frequently, and when they are, they're typically described as minor or temporary. There's a clear endorsement of the medication.\n",
        "\n",
        "10.0: Almost universally positive, with users expressing great satisfaction and often describing the medication as life-changing or highly effective. Side effects are rarely mentioned, and if they are, they are downplayed or considered insignificant compared to the benefits.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Based on this more detailed analysis, the sentiment threshold is still around the 6.0 to 7.0 range, the sentiment leans to be more positive closer to 7.0.\n",
        "\n",
        "Negative Sentiment: Ratings 1.0 to 6.0\n",
        "\n",
        "Positive Sentiment: Ratings 7.0 to 10.0\n",
        "\n",
        "The key difference is the increased nuance we see in the 4.0, 5.0, and 6.0 ratings. These are not clearly negative in the same way as the 1.0-3.0 ratings, but they represent a mixed bag of experiences where the negative aspects often outweigh or significantly detract from the positive ones. The 7.0 rating represents the point where the balance generally tips towards a positive overall experience, despite potential drawbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "DrWX90RER4"
      },
      "source": [
        "# Create histogram with two class distributions\n",
        "plt.figure(figsize=(12, 8), dpi=400)\n",
        "\n",
        "# Define the bins for negative (1-6) and positive (7-10) classes\n",
        "bins = [0, 6.5, 10]  # Using 6.5 as the boundary to properly separate 6 and 7\n",
        "counts, bins, patches = plt.hist(df[\"rating\"], bins=bins, edgecolor=\"black\")\n",
        "total = len(df[\"rating\"])\n",
        "\n",
        "# Color the bars differently\n",
        "patches[0].set_facecolor(\"salmon\")  # Negative class (1-6)\n",
        "patches[1].set_facecolor(\"lightgreen\")  # Positive class (7-10)\n",
        "\n",
        "# Calculate maximum y value needed\n",
        "max_count = max(counts)\n",
        "y_margin = max_count * 0.15  # Add 15% margin for labels\n",
        "\n",
        "# Set y-axis limit\n",
        "plt.ylim(0, max_count + y_margin)\n",
        "\n",
        "\n",
        "# Add percentage labels on top of each bar\n",
        "for i in range(len(counts)):\n",
        "    percentage = (counts[i] / total) * 100\n",
        "    plt.text(bins[i], counts[i], f\"{counts[i]:,.0f}\\n({percentage:.1f}%)\", va=\"bottom\")\n",
        "\n",
        "plt.title(\"Distribution of Drug Ratings by Class (Negative: 1-6, Positive: 7-10)\")\n",
        "plt.xlabel(\"Rating Classes\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Customize x-axis labels\n",
        "plt.xticks([3.25, 8.25], [\"Negative\\n(1-6)\", \"Positive\\n(7-10)\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed statistics\n",
        "print(\"\\n--- Class Distribution Statistics ---\")\n",
        "print(f\"Total reviews: {total:,}\")\n",
        "print(f\"Negative class (1-6): {counts[0]:,.0f} reviews ({(counts[0]/total)*100:.1f}%)\")\n",
        "print(f\"Positive class (7-10): {counts[1]:,.0f} reviews ({(counts[1]/total)*100:.1f}%)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "56lTfAIOuH"
      },
      "source": [
        "1. **Purpose**: This histogram shows how the drug reviews are distributed when split into two classes based on ratings:\n",
        "   - Negative class: Ratings from 1-6\n",
        "   - Positive class: Ratings from 7-10\n",
        "\n",
        "2. **Visual Elements**:\n",
        "   - Red (salmon) bar: Represents negative reviews (ratings 1-6)\n",
        "   - Green bar: Represents positive reviews (ratings 7-10)\n",
        "   - Each bar shows both the count and percentage of reviews\n",
        "\n",
        "3. **Key Findings**:\n",
        "   - Total Dataset Size: 110,811 reviews\n",
        "   - Negative Reviews (1-6): 37,173 reviews (33.5%)\n",
        "   - Positive Reviews (7-10): 73,638 reviews (66.5%)\n",
        "\n",
        "4. **Interpretation**:\n",
        "   - The data is imbalanced, with about twice as many positive reviews as negative ones\n",
        "   - Roughly 2/3 of all reviews are positive (7-10 rating)\n",
        "   - Only 1/3 of reviews are negative (1-6 rating)\n",
        "\n",
        "5. **Implications**:\n",
        "   - This imbalance suggests people are more likely to leave positive reviews\n",
        "   - We might need to consider techniques to handle class imbalance (like oversampling, undersampling, or class weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "CDfMwYSgQ9"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "The dataset has been mentioned to be clean. Therefore, we just need to remove the row index. This has to be done during testing as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "1IqN87MGOq"
      },
      "source": [
        "# Check and remove Unnamed: 0 column if it exists\n",
        "if \"Unnamed: 0\" in df.columns:\n",
        "    print(\"\\n--- Removing 'Unnamed: 0' column ---\")\n",
        "    print(f\"Original columns: {df.columns.tolist()}\")\n",
        "    df = df_train.drop(\"Unnamed: 0\", axis=1)\n",
        "    print(f\"Updated columns: {df.columns.tolist()}\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "QRwMNsE7F6"
      },
      "source": [
        "## Feature Engineering\n",
        "    - Prepare features for a distilBERT feature extracto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "JDSbhRqmUL"
      },
      "source": [
        "print(\"\\n=== Starting Data Preprocessing ===\")\n",
        "\n",
        "# Remove Unnamed column if it exists\n",
        "if \"Unnamed: 0\" in df.columns:\n",
        "    df = df.drop(\"Unnamed: 0\", axis=1)\n",
        "\n",
        "# Create binary sentiment labels (0 for ratings 1-6, 1 for ratings 7-10)\n",
        "print(\"\\nCreating sentiment labels...\")\n",
        "df[\"sentiment_label\"] = (df[\"rating\"] >= 7).astype(int)\n",
        "\n",
        "# For DistilBERT, we'll just use the raw review text\n",
        "# No need for text preprocessing as the model's tokenizer will handle it\n",
        "\n",
        "print(\"\\n=== Basic Preprocessing Complete ===\")\n",
        "print(f\"Total reviews: {len(df):,}\")\n",
        "print(f\"Positive reviews (rating >= 7): {df['sentiment_label'].sum():,}\")\n",
        "print(f\"Negative reviews (rating < 7): {len(df) - df['sentiment_label'].sum():,}\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "print(\"\\n=== Preparing Data Split ===\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(\"date\")\n",
        "\n",
        "# Calculate split points (80% train, 10% val, 10% test)\n",
        "train_end_idx = int(len(df) * 0.8)\n",
        "val_end_idx = int(len(df) * 0.9)\n",
        "\n",
        "# Split the data\n",
        "train_df = df[:train_end_idx]\n",
        "val_df = df[train_end_idx:val_end_idx]\n",
        "test_df = df[val_end_idx:]\n",
        "\n",
        "# Print split statistics\n",
        "print(\"\\n=== Data Split Statistics ===\")\n",
        "print(f\"Training set: {len(train_df):,} reviews\")\n",
        "print(\n",
        "    f\"  Positive: {train_df['sentiment_label'].sum():,} ({train_df['sentiment_label'].mean()*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Negative: {len(train_df) - train_df['sentiment_label'].sum():,} ({(1-train_df['sentiment_label'].mean())*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Date range: {train_df['date'].min().strftime('%Y-%m-%d')} to {train_df['date'].max().strftime('%Y-%m-%d')}\"\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation set: {len(val_df):,} reviews\")\n",
        "print(\n",
        "    f\"  Positive: {val_df['sentiment_label'].sum():,} ({val_df['sentiment_label'].mean()*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Negative: {len(val_df) - val_df['sentiment_label'].sum():,} ({(1-val_df['sentiment_label'].mean())*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Date range: {val_df['date'].min().strftime('%Y-%m-%d')} to {val_df['date'].max().strftime('%Y-%m-%d')}\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTest set: {len(test_df):,} reviews\")\n",
        "print(\n",
        "    f\"  Positive: {test_df['sentiment_label'].sum():,} ({test_df['sentiment_label'].mean()*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Negative: {len(test_df) - test_df['sentiment_label'].sum():,} ({(1-test_df['sentiment_label'].mean())*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"  Date range: {test_df['date'].min().strftime('%Y-%m-%d')} to {test_df['date'].max().strftime('%Y-%m-%d')}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "QAiQKKipZO"
      },
      "source": [
        "## Data Splitting\n",
        "   - Split data into training and validation sets\n",
        "   - Consider temporal splits given the date column\n",
        "   - Ensure balanced distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "mdVUZJSjyd"
      },
      "source": [
        "# Can't fully reuse part 1 sadly.\n",
        "def train_model(\n",
        "    model,\n",
        "    dataloaders,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=10,\n",
        "    model_name=\"Model\",\n",
        "    class_weights=None,\n",
        "    early_stopping_patience=3,\n",
        "    gradient_accumulation_steps=1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a model for sentiment classification with advanced training features.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        dataloaders: Dictionary containing 'train' and 'valid' dataloaders\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        num_epochs: Number of training epochs\n",
        "        model_name: Name for saving the model\n",
        "        class_weights: Weights for handling class imbalance\n",
        "        early_stopping_patience: Number of epochs to wait before early stopping\n",
        "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
        "    \"\"\"\n",
        "    # Initialize tracking variables\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    train_metrics = {\"loss\": [], \"acc\": [], \"f1\": []}\n",
        "    val_metrics = {\"loss\": [], \"acc\": [], \"f1\": []}\n",
        "\n",
        "    # Move model and criterion to device\n",
        "    model = model.to(device)\n",
        "    if class_weights is not None:\n",
        "        class_weights = class_weights.to(device)\n",
        "        criterion = criterion(weight=class_weights)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        train_pbar = tqdm(\n",
        "            dataloaders[\"train\"], desc=\"Training\", position=1, leave=False\n",
        "        )\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_pbar):\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss = loss / gradient_accumulation_steps  # Scale loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Track metrics\n",
        "            running_loss += loss.item() * gradient_accumulation_steps\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix(\n",
        "                {\n",
        "                    \"loss\": f\"{loss.item():.4f}\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_loss = running_loss / len(dataloaders[\"train\"])\n",
        "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "        epoch_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "        train_metrics[\"loss\"].append(epoch_loss)\n",
        "        train_metrics[\"acc\"].append(epoch_acc)\n",
        "        train_metrics[\"f1\"].append(epoch_f1)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_val_preds = []\n",
        "        all_val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(\n",
        "                dataloaders[\"valid\"], desc=\"Validation\", position=1, leave=False\n",
        "            )\n",
        "\n",
        "            for inputs, labels in val_pbar:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_val_preds.extend(preds.cpu().numpy())\n",
        "                all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(dataloaders[\"valid\"])\n",
        "        val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
        "        val_f1 = f1_score(all_val_labels, all_val_preds, average=\"weighted\")\n",
        "\n",
        "        val_metrics[\"loss\"].append(val_loss)\n",
        "        val_metrics[\"acc\"].append(val_acc)\n",
        "        val_metrics[\"f1\"].append(val_f1)\n",
        "\n",
        "        # Log metrics\n",
        "        logger.info(\n",
        "            f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}\"\n",
        "        )\n",
        "        logger.info(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Model checkpointing based on F1 score\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            logger.info(\"Saving best model checkpoint...\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                    \"best_val_f1\": best_val_f1,\n",
        "                },\n",
        "                f\"best_{model_name}.pth\",\n",
        "            )\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(train_metrics, val_metrics, model_name)\n",
        "\n",
        "    return train_metrics, val_metrics\n",
        "\n",
        "\n",
        "def plot_training_history(train_metrics, val_metrics, model_name):\n",
        "    \"\"\"Plot training and validation metrics history.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0].plot(train_metrics[\"loss\"], label=\"Train\")\n",
        "    axes[0].plot(val_metrics[\"loss\"], label=\"Val\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1].plot(train_metrics[\"acc\"], label=\"Train\")\n",
        "    axes[1].plot(val_metrics[\"acc\"], label=\"Val\")\n",
        "    axes[1].set_title(\"Accuracy\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Plot F1 score\n",
        "    axes[2].plot(train_metrics[\"f1\"], label=\"Train\")\n",
        "    axes[2].plot(val_metrics[\"f1\"], label=\"Val\")\n",
        "    axes[2].set_title(\"F1 Score\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.suptitle(f\"Training History - {model_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create the sentiment classifier model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=2, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Initialize the sentiment classifier\n",
        "        Args:\n",
        "            n_classes (int): Number of output classes\n",
        "            dropout_rate (float): Dropout rate for regularization\n",
        "        \"\"\"\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "\n",
        "        # Load pre-trained DistilBERT\n",
        "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        # Freeze BERT parameters (optional - comment out if you want to fine-tune everything)\n",
        "        # for param in self.distilbert.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(384, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(96, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Args:\n",
        "            input_ids: Tokenized input sequences\n",
        "            attention_mask: Attention mask for padding\n",
        "        \"\"\"\n",
        "        # Get DistilBERT outputs\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Use the [CLS] token representation\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "\n",
        "        # Pass through the classifier\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "\n",
        "# Initialize training components\n",
        "def initialize_training(train_df, val_df, batch_size=16, max_length=512):\n",
        "    \"\"\"\n",
        "    Initialize all components needed for training\n",
        "    Args:\n",
        "        train_df (DataFrame): Training dataframe\n",
        "        val_df (DataFrame): Validation dataframe\n",
        "        batch_size (int): Batch size for training\n",
        "        max_length (int): Maximum sequence length\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = DrugReviewDataset(\n",
        "        train_df[\"review\"].values,\n",
        "        train_df[\"sentiment_label\"].values,\n",
        "        tokenizer,\n",
        "        max_length,\n",
        "    )\n",
        "\n",
        "    val_dataset = DrugReviewDataset(\n",
        "        val_df[\"review\"].values, val_df[\"sentiment_label\"].values, tokenizer, max_length\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Calculate class weights for imbalanced dataset\n",
        "    total_samples = len(train_df)\n",
        "    neg_samples = (train_df[\"sentiment_label\"] == 0).sum()\n",
        "    pos_samples = (train_df[\"sentiment_label\"] == 1).sum()\n",
        "\n",
        "    class_weights = torch.tensor(\n",
        "        [total_samples / (2 * neg_samples), total_samples / (2 * pos_samples)]\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize model\n",
        "    model = SentimentClassifier().to(device)\n",
        "\n",
        "    # Initialize optimizer with weight decay\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False\n",
        "    )\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True\n",
        "    )\n",
        "\n",
        "    # Initialize loss function with class weights\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    return {\n",
        "        \"device\": device,\n",
        "        \"model\": model,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"criterion\": criterion,\n",
        "        \"class_weights\": class_weights,\n",
        "    }\n",
        "\n",
        "\n",
        "training_components = initialize_training(train_df, val_df)\n",
        "\n",
        "# Create dataloaders dictionary for the train_model function\n",
        "dataloaders = {\n",
        "    \"train\": training_components[\"train_loader\"],\n",
        "    \"valid\": training_components[\"val_loader\"],\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "train_metrics, val_metrics = train_model(\n",
        "    model=training_components[\"model\"],\n",
        "    dataloaders=dataloaders,\n",
        "    criterion=training_components[\"criterion\"],\n",
        "    optimizer=training_components[\"optimizer\"],\n",
        "    scheduler=training_components[\"scheduler\"],\n",
        "    num_epochs=10,\n",
        "    model_name=\"DrugReviewSentiment\",\n",
        "    class_weights=training_components[\"class_weights\"],\n",
        "    early_stopping_patience=3,\n",
        "    gradient_accumulation_steps=4,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "3tur77deJS"
      },
      "source": [
        "## Model Development\n",
        "### Direct Training approach\n",
        "- Define target variable (likely rating)\n",
        "- Build and train models\n",
        "- Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "9fl0EswBkd"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "pXIxmKjxJr"
      },
      "source": [
        "### Indirect Training approach\n",
        "- Define indirect signals/proxies\n",
        "- Build and train models\n",
        "- Compare with direct training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "0Ql4czNgYC"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "ibWP1EUlZX"
      },
      "source": [
        "## Model Comparison\n",
        "   - Compare direct vs indirect training approaches\n",
        "   - Analyze pros and cons of each method\n",
        "   - Discuss real-world applicability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "PFd7PzIchR"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "Whd9Jk0EPs"
      },
      "source": [
        "## Results and Discussion\n",
        "   - Present key findings\n",
        "   - Discuss limitations\n",
        "   - Suggest improvements"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}