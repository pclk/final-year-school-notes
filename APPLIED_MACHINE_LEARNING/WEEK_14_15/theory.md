# deck: AML AI Ethics

The reason we require -Model AI Governance Framework- (also known as "-Model Framework-") is because the -speed- of -AI's advancement-.

Model Framework translates -ethical principles- into -practical recommendations- that organizations could adopt.

## Parts 

The 4 parts of Model Framework are -Internal Governance structures & Measures-, -Determining the level of human involvement in AI-, -Operations Management- and -Stakeholder Interaction & Communication-

-Internal Governance structure & Measures- refers to -clear roles & responsibilities- in organizations. 1.-SOPs- to 1.-monitor- and 1.-manage- 1.-risks-, and -Staff training-.

-Determining the level of human involvement in AI\-Augmented Decision\-making- refers to 1.-appropriate degrees- of 1.-human involvement-, and 2.-minimizing- the 2.-risk- of 2.-harm- to 2.-individuals-.

-Operations Management- refers to 1.-minimizing bias- in 1.-data- and 1.-model- and a -risk-based approach- to measures such as -explainability-, -robustness- and -regular tuning-.

-Stakeholder Interaction & Communication- refers to making 1.-AI policies known- to 1.-users-, allowing 2.-users- to 2.-provide feedback-, and making -communications understandable-.

## Not limited to

Model framework is not limited to -algorithms-, -technology-, -sectors-, -scale- and -business logic-.

-Algorithm- agnostic because it -doesn't focus on specific AI methodology-.

-Technology- agnostic because it -doesn't focus on specific systems, software, language or storage-.

-Sector- agnostic because it -serves as baseline set of considerations and measures-.

## Guiding principles

The decision\-making process should be -explainable-, -transparent- and -fair- as much as possible. This helps build -trust- and -confidence- in AI.

AI solutions should be -human\-centric-, by 1.-protecting- the 1.-interest- of 1.-human beings-, including their -well\-being- and -safety-.

## Assumptions

Model framework aims to discuss -good data management practices- in general.

Model framework is mainly applicable to -machine learning models-, not to -pure decision tree\-driven AI models-.

Model framework does not address the risk of 1.-failure- due to 1.-cyber\-attacks-. 

Model framework adoption does not -absolve- organizations from -compliance- from current 1.-laws- and 1.-regulation-. However, it could demonstrate -accountability-. 

## Definition

-AI- refers to a set of technologies that seek to -simulate human traits-, to produce an 1.-output- or 1.-decision-.

-AI solution providers- don't just develop for -B2C-, but also -B2B2C-

-Organizations- refer to -business entities- that 1.-adopt- or 1.-deploy- -AI solutions- in their operations.

-Individuals-, -consumers- and -customers- refers to persons whom -organizations- intend to -supply AI products/services- to, or have 1.-purchased- the 1.-AI products/services-

## Internal Governance Structures and Measures

The -appropriate personnel- or -departments- should be responsible for and oversight of the various stages and activities involved in -AI deployment-  

If necessary and possible, consider establishing a -coordinating body-, having -relevant expertise- and -proper representation- from across the organisation.

Personnel and/or departments having -internal AI governance functions- should be fully aware of their 1.-roles- and 1.-responsibilities-, be -properly trained-, and be provided with the 2.-resources- and 2.-guidance- needed for them to discharge their duties.

## Key roles & responsibilities

Someone in the organization should use any existing -risk management framework- and applying -risk control measures- to 1.-assess- and 1.-manage- the 1.-risks- of deploying AI, decide on the 2.-appropriate level of human involvement- in 2.-AI\-augmented decision\-making-, and manage the 3.-AI model training- and 3.-selection process-.

When assessing the -risks of deploying AI-, include any potential -adverse impact- on the -individuals- (e.g. who are most -vulnerable-, how are they -impacted-, how to -assess the scale of the impact-, how to get -feedback- from those impacted, etc.).

Someone should -maintain-, -monitor-, -document- and -review- the -AI models- that have been deployed, with a view to taking -remediation measures- where needed.

Someone should review -communications channels- and -interactions- with -stakeholders- to provide -disclosure- and -effective feedback channels-

Someone should ensure -relevant staff- dealing with -AI systems- are -properly trained-. 

Staff who are -working- and -interacting- directly with -AI models- may need to be trained to -interpret AI model output- and -decisions- and to -detect- and -manage bias- in data. 

Staff whose work -indirectly deals- with the -AI system- should be trained to be at least -aware- and -sensitive- to the -benefits-, -risks- and -limitations- when using AI, so that they know when to alert -subject\-matter experts- within their organisations


## Risk Management and Internal Controls

Organizations should use reasonable efforts to ensure that the -datasets- used for -AI model training- are adequate for the intended purpose, and to assess and manage the risks of -inaccuracy- or -bias-, as well as reviewing -exceptions- identified during model training. 

Organizations should strive to understand the ways in which datasets may be -biased- and address this in their -safety measures- and -deployment strategies-.

Organizations should establish -monitoring- and -reporting systems- as well as processes to ensure that the -appropriate level of management- is aware of the -performance- of and other issues relating to the deployed AI.

Organizations should include -autonomous monitoring- to effectively scale -human oversight-.

Organizations should design AI systems to report on the -confidence level- of their predictions, 

Organizations should design -explainability features- that focus on why the AI model had a certain level of -confidence-.

Organizations should ensure proper -knowledge transfer- whenever there are changes in -key personnel- involved in AI activities, to reduce the risk of gaps in -internal governance-.

Organizations should review the -internal governance structure- and -measures- when there are significant changes to -organizational structure- or -key personnel- involved.

Organizations should -periodically review- the internal governance structure and measures to ensure their continued -relevance- and -effectiveness-.

## Examples

### CUJO AI

-Research Board- consisting of the -Chief Technology Officer-, the -Head of Labs- and the -Chief Data Scientist-, that approves the -AI development- and -deployment-.

-Chief Technology Officer- oversees -four technical teams- which consists of more than -100 employees-.

-Research Team-: performs -data analysis-, -research- and develop -Machine Learning models- and -AI algorithms-

-Engineering Team-: builds -software-, -cloud services- and -applications-

-Operation Team-: deploys the -AI model- and -upgrade platform-

-Delivery Team-: engages with -operators- and -integrate services-.

An -Architecture Steering Group- consisting of the -Chief Technology Officer-, -Chief Architect Officer- and -lead engineers-, ensures the -robustness- of the -AI/ML models- before -deployment-.

The -ASG- has -bi\-weekly meetings- where the -research team- shares its -findings- on the -ML models- and -AI algorithms- (e.g. -data-, -approach- and -assumptions-).

-PhD\-level employees- oversee the -AI development- and -deployment process-, and strive to implement -academic review standards- for each -new feature development-.

Employee ethical principles are to conduct -business ethically and honestly- across all offices, base decisions on -integrity-, -fairness- and -sound judgment-, not allow -unethical conduct- from any 1.-employee- or 1.-affiliate- and never compromise -principles- for -short\-term gains-.

### MASTERCARD

Established a -Governance Council- chaired by -Executive VP of AI Center of Excellence-, along with Chief -Data- Officer, Chief -Privacy- Officer, Chief -Information Security- Officer, -data scientists-, and -business representatives-, to 1.-review- and 1.-approve- AI implementations that are -high risk-.

Chief -Data- and Chief -Privacy- Officer will ensure AI implementation proposal has -data fit for AI-, -AI used for ethical purpose-, -impacts to individuals are appropriate-, and -potential harms are sufficiently mitigated-.

-Chief Information Security Officer- will ensure that -security by design- is implemented.

-Data Science teams- that build and implement AI maintain continuous dialogue with two offices: The -Data Office- and The -Privacy Office-. This ensures -continued information sharing about required governance-, and -ongoing communication about the lifecycle of AI application implementations-

Conducts -initial risk scoring- by evaluating -alignment with corporate initiatives-, -data types and sources-, and -impact on individuals-.

Identify -potential mitigations- to reduce level of risk from -collecting data-, or -potential biases-.

## Determining the Level of Human Involvement in AI-Augmented Decision-Making

-Commercial objectives- should be weighed against -AI risks- in decision\-making. When weighing AI risks, it should be guided by -corporate values-, which should reflect -societal norms of the country-. -Multi national companies- should consider -differences in societal norms-.

-Risk levels- vary based on -where AI is deployed-.

-Human\-in\-the\-loop- suggests that -human oversight is active and involved-, with the -human retaining full control- and the -AI only providing recommendations or input-. Decisions cannot be exercised without -affirmative actions by the human-, such as a -human command to proceed- with a given decision. However, this requires the AI to provide -enough information for human to make an informed decision-, like the -factors considered-, -weights of the factors-, -correlations-.

-Human\-out\-of\-the\-loop- suggests that there is -no human oversight- over the execution of decisions, and the -AI system has full control- without the -option of human override-. For example, -product recommendation engine-, -airline demand forecaster-.

-Human\-over\-the\-loop- or -human\-on\-the\-loop- suggests that human oversight is involved to the extent that the -human is in a monitoring or supervisory role-. Human can -take over control- when the Al model encounters -unexpected or undesirable events-. For example, -driver controlling GPS navigation system with route\-planning-.

## Risk Assessment Matrix

The -risk assessment matrix- is structured along two axes. The -probability- and -severity of harm- as a result of the decision made impacting the user.

### Online Retail Product Recommendations

Using -AI- to automate the recommendation of food products to individuals is a -low probability, low severity risk assessment-.

-Low probability and severity assessment- points to -human\-out\-of\-the\-loop-.

The -probability of harm- depends on the -efficiency- and -efficacy- of the AI solution.

Regularly -review- and -re\-assess- the probability and severity of harm as -societal norms- evolve.

### SUADE LABS

The -probability of harm- depends on -degree of domain knowledge- required to accurately interpret the results of AI.

The -severity of harm- depends on the -cost of non\-compliance- to regulation.

Using -AI- to generate required regulatory data is a -high probability, high severity risk assessment-.

-High probability and severity assessment- points to -human\-in\-the\-loop-. Should also favour -false positive- over -false negatives-. Though if -user research- indicates opposing preference, allow the option to tune to personal preference.

### GRAB

In risk assessment, the -feasibility- of the -human\-in\-the\-loop- should be considered.

Using -AI- to optimize trip allocations is a -low probability, low severity risk assessment-.

## Clear section

-Datasets- used for building models may come from -multiple sources-, and could include both -personal- and -non\-personal data-.

The -quality- and -selection- of data from each of these sources are critical to the -success- of an AI solution.

If a model is built using -biased-, -inaccurate- or -non\-representative data-, the risks of -unintended discriminatory decisions- from the model will increase.

## Good data accountability practices

Understanding the -lineage of data- means knowing where the -data originally came from-, how it was -collected-, -curated-, and -moved within the organisation-, and how its -accuracy- is maintained over time.

The 3 types of data lineage are -Backward data lineage-, -Forward data lineage- and -End\-to\-end data lineage-

-Backward data lineage- looks at the data from its -end\-use- and -backdating it to its source-.

-Forward data lineage- begins at the -data's source- and follows it through to its -end\-use-.

-End\-to\-end data lineage- combines the two and looks at the entire solution from both the -data's source to its end\-use- and from its -end\-use to its source-.

Keeping a -data provenance record- allows an organisation to -ascertain the quality of the data- based on its -origin- and -subsequent transformation-, -trace potential sources of errors-, -update data-, and -attribute data to their sources-.

If the -origin of data- is difficult to establish, the organization needs to -reconsider the risks- of using such data.

-Good data accountability practices- include -understanding the lineage of data-, -ensuring data quality-, -minimizing bias-, -dataset splitting-, and -periodic reviews and updates-.

### Ensuring data quality

The -accuracy- of the dataset, in terms of how well the -values- in the dataset match the -true characteristics- of the entities described by the dataset;

The -completeness- of the dataset, both in terms of -attributes- and -items-;

The -veracity- of the dataset, which refers to how -credible- the data is, including whether the data originated from a -reliable source-;

How -recently- the dataset was -compiled- or -updated-;

The -relevance- of the dataset and the -context for data collection-, as it may affect the -interpretation- and -reliance- on the data for the intended purpose;

The -integrity- of the dataset that has been joined from multiple datasets, which refers to how well -extraction- and -transformation- have been performed;

The -usability- of the dataset, including how well the dataset is -structured- in a -machine\-understandable form-;

-Human interventions- (e.g. if any human has -filtered-, -applied labels-, or -edited- the data).

### Bias in data

#### Identification

The -2 common types of bias in data- include -Selection bias- and -Measurement bias-.

-Selection bias- occurs when the -training data is not representative of the actual environment the model functions in-.

The -2 common examples of selection bias- include -omission bias- and -stereotype bias-.

-Omission bias- describes the -omission of certain characteristics from the dataset-. 

An example of -Omission bias- is a -dataset consisting only of Asian faces but used for a population that includes non\-Asians-. 

An example of -stereotype bias- is a -dataset of vehicle types representing types of transportation available but data collection was within CBD weighted to cars, buses and motorcycles but not bicycles-.

-Measurement bias- occurs when the -data collection device causes the data to be systematically skewed in a particular direction-.

An example of -Measurement bias- is a -pictures with camera that can't see a colour-.

#### Mitigation

Have a -heterogeneous dataset-, collecting data from a -variety of reliable sources-.

Avoid -premature removal of data attributes- to allow for -identifying and addressing of bias-.

### Dataset splitting

-Different datasets- are required for -training-, -testing-, and -validation-.

The model could also be checked for -systematic bias- by -testing it on different demographic groups- to observe whether any groups are being -systematically advantaged or disadvantaged-.

If can't test model on different demographic groups, maybe because -small dataset- or -transfer learning-, then be -aware of systematic bias- and place -appropriate safeguards-.

### Periodic reviews and updates

-Dataset- with -new updated input data- obtained from production should be checked for -potential bias- if it has already gone through the model. 

-Review datasets periodically- to ensure -accuracy-, -quality-, -currency-, -relevance- and -reliability-.

### Examples

#### SUADE LABS

-Obtain and update regulatory data- only from -relevant regulators-.

-Tag datasets- with -metadata- to -trace to data source- when needed, such as where -inconsistencies- are found.

-Document and store- which -datasets- were used in an -AI model-.

Use as many -individuals- as practicable to -tag data- to reduce -tagger bias-. 

Develop a -tagging system- to facilitate the -annotation of data-.

-Verify- that the -data schema- accurately represents the -data from the source-, to ensure there are no -errors- in factors such as the -data's formatting and content-.

#### PYMETRICS:

-Validate- generated -training data- with your -client's team-.

Use -objective features- based on -established research- that are generally -stable across demographic groups-.

Do not -disadvantage people- on the basis of their -demographic features-.

The -US' Equal Employment Opportunity Commission- states that the -selection rate- for any -legally protected group- must be at least -80%- of the -selection rate- for the -majority group-.

An example of the US' Equal Employment Opportunity Commission is if -100 men- and -100 women- are screened and -50 men- are selected, at least -40 women- must be selected.

After -model deployment-, test on -real individuals- for -adverse impact-, and -revisit- for -long\-term impacts- of the -AI predictions-.


## Best practices for Algorithms and Models

The -5 best practices for Algorithms and Models- are -Explainability-, -Repeatability-, -Robustness-, -Regular Tuning- and -Traceability-.

### Explainability

-Explainability- is achieved by explaining how -deployed AI models' algorithms function- and how the -model predictions are utilized to make decisions-.

The purpose of being able to -explain predictions made by AI- is to build -understanding- and -trust-.

An algorithm deployed in an AI solution is said to be -explainable- if -how it functions- and -how it arrives at a particular prediction- can be explained.

When an -algorithm cannot be explained-, understanding and trust can still be built by explaining how -predictions play a role in the decision\-making process-.

-Documenting- how the -model training- and -selection processes- are conducted, the -reasons for which decisions are made-, and -measures taken to address identified risks- will enable the organisation to provide an account of the decisions.

When using -Automated Machined Learning tools-, organizations should consider the -transparency-, -explainability- and -traceability- of the automated machine learning approach.

Include -design- (why certain decisions were made) and -expected behaviour- as -product labels- and -documentations- for accountability and understanding.

If -AI system- was obtained from -3rd\-party AI solution provider-, request assistance to explain how the solution functions.

Use -supplementary explanation tools- to help make -underlying rationale of AI output- more -interpretable to users-, and improve -explainability-.

### Repeatability

-Repeatability- refers to the ability to -consistently perform an action- or -make a decision-, given the -same scenario-.

While -repeatability- (of results) is not equivalent to -explainability- (of algorithm), some degree of -assurance of consistency- in performance could provide AI users with a larger degree of -confidence-.

Conducting -repeatability assessments- for -commercial deployments- in -live environments- to ensure that deployments are repeatable.

-Counterfactual fairness testing- ensures that a model's decisions are the same in both the -real world- and in a -counterfactual world- where -attributes deemed sensitive- (such as -race- or -gender-) are altered.

Assessing how -exceptions- can be identified and handled when -decisions are not repeatable-, e.g. when -randomness- has been introduced by design.

Helpful if -AI models- can highlight -situations with new variables- previously not considered to a human.

Identifying and accounting for -changes over time- to ensure that -models trained on time\-sensitive data- remain relevant.

### Robustness

Ensuring that -deployed models- are sufficiently -robust- will contribute towards building -trust- in the -AI system-.

-Robustness- refers to the ability of a computer system to -cope with errors- during -execution- and -erroneous input-.

-Robustness- is assessed by the degree to which a system or component can -function correctly- in the presence of -invalid input- or -stressful environmental conditions-.

-Continual learning- is when models -changes its learned parameters- after being -deployed into production-.

-Continual learning- can be -unpredictable- since the -input data- has changed.

### Regular tuning

Establish an -internal process- to perform -regular model tuning- to ensure that -deployed models- cater for -changes to customer behaviour- over time.

-Tune models- when -commercial objectives-, -risks- or -corporate values- change.

### Traceability

-AI\-augmented decisions- are -traceable- if -decisions of the model- are documented.

-Model training- is traceable if -dataset- and -training process- of AI model are documented.

-Traceability- facilitates -transparency- and -explainability-, and also can be used for -troubleshooting- or a -source of input data-

A -black box recorder- captures all -input data streams-. For example, tracking a -car's position-, -technical problems- and -requests to take over control- of the car.

To promote -traceability-, build an -audit trail- to document -model training- and -AI\-augmented decision-, implementing a -black box recorder-, and storing data appropriately. 

-Traceability measures- may lead to a -large volume of activity data-. Therefore organizations should reconsider prioritizing what -product features- require traceability, and which -traceability measure- to apply.

Organization considering the implementation of -traceability- should base it on -risk assessment matrix-, -length of time the model has been used-, and -regulatory needs- of their industry.
