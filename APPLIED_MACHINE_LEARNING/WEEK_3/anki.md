# The {{Supervised Classification}} learning algorithms are {{K-Nearest Neighbour algorithm}}, {{Logistic regression}}, {{Decision tree}}, {{Support Vector Machine}}.

# In {{KNN}}, the {{K}} parameter defines the amount of {{nearest neighbours}} considered.

# Accuracy can tell how well the algorithm can {{predict}} but it does not give information on {{how to improve the model}}.

# Actual {{Negative}}, Predicted {{Negative}} = {{True Negative}}. 

# Actual {{Negative}}, Predicted {{Positive}} = {{False Positive}}. 

# Actual {{Positive}}, Predicted {{Negative}} = {{False Negative}}. 

# Actual {{Positive}}, Predicted {{Positive}} = {{True Positive}}. 

# Usually, when Precision {{increases}}, Recall will {{decrease}}.

# The metrics {{precision}} and {{recall}} usually have an {{inverse relationship}}.

# {{Precision}} is optimized when cost of failure is {{low}}

# {{Recall}} is optimized when cost of failure is {{high}}.

# F1 Score considers both {{Precision}} and {{Recall}}, finding a {{harmonic mean}}.

# When we need to visualize the {{performance}} of the {{multi-class classification}} parameter, we use the {{AUC}} and {{ROC}}

# To draw the {{AUC}} and {{ROC}}, we use the {{True Positive Rate}} and {{False Positive Rate}}.

# The term {{sensitivity}} also means {{True Positive Rate}}.

# The term {{1 - specificity}} also means {{False Positive Rate}}

# {{ROC}} shows a trade-off between {{TPR}} and {{FPR}} for every possible {{cut-off}} ({{threshold}})

# {{Higher}} is better for {{AUC}}.

# The {{Logistic Regression Curve}} is also known as the {{Sigmoid Curve}}.

# In {{decision tree}}, a {{node}} is represented as a {{feature}}, a {{link}} is represented as a {{decision}}, and a {{outcome}} is represented as a {{leaf}}.

# In {{SVM}}, we don't want the data {{nearest to the line}} to affect the decision making process.
