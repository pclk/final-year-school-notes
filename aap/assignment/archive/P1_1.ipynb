{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jukit_cell_id": "0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing search term: full stack engineer\n",
      "==================================================\n",
      "\n",
      "Scraping Glassdoor jobs for US - Search: full stack engineer\n",
      "Found 0 previously scraped jobs\n",
      "First few scraped job links: []\n",
      "\n",
      "Processing page 1 for US. Current jobs: 0/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 730\u001b[0m\n\u001b[1;32m    727\u001b[0m log_file\u001b[38;5;241m.\u001b[39mwrite(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m     jobs \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_glassdoor_jobs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjobs_per_country\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mJOBS_PER_COUNTRY\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully scraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(jobs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m jobs from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message)\n",
      "Cell \u001b[0;32mIn[1], line 379\u001b[0m, in \u001b[0;36mscrape_glassdoor_jobs\u001b[0;34m(query, country, jobs_per_country)\u001b[0m\n\u001b[1;32m    374\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Current jobs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_job_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjobs_per_country\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Current URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m )\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to load job cards on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:382\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    380\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    137\u001b[0m         method,\n\u001b[1;32m    138\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from random import uniform\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure and return the Chrome WebDriver with appropriate options.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_job_details(soup, driver, job_card):\n",
    "    try:\n",
    "        job_data = {\n",
    "            \"country\": \"\",\n",
    "            \"job_description\": \"\",\n",
    "            \"location\": \"\",\n",
    "            \"salary\": \"\",\n",
    "            \"job_title\": \"\",\n",
    "            \"job_link\": \"\",\n",
    "        }\n",
    "\n",
    "        # Get the job link and store it\n",
    "        job_link = job_card.find(\"a\", class_=\"JobCard_trackingLink__HMyun\")\n",
    "        if job_link and job_link.get(\"href\"):\n",
    "            href = job_link.get(\"href\")\n",
    "            if href.startswith(\"/\"):\n",
    "                base_url = \"https://www.glassdoor.com\"\n",
    "                if \"glassdoor.sg\" in driver.current_url:\n",
    "                    base_url = \"https://www.glassdoor.sg\"\n",
    "                elif \"glassdoor.co.in\" in driver.current_url:\n",
    "                    base_url = \"https://www.glassdoor.co.in\"\n",
    "                job_data[\"job_link\"] = base_url + href\n",
    "            else:\n",
    "                job_data[\"job_link\"] = href\n",
    "\n",
    "        # Extract salary by iterating through possible salary elements\n",
    "        try:\n",
    "            # Find all salary elements in the job cards\n",
    "            salary_elements = driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"[data-test='detailSalary']\"\n",
    "            )\n",
    "\n",
    "            # Get the current job's href to match with the correct salary\n",
    "            current_job_href = job_link.get(\"href\") if job_link else None\n",
    "\n",
    "            if current_job_href:\n",
    "                # Find the matching salary element for this job card\n",
    "                salary_found = False\n",
    "                for salary_element in salary_elements:\n",
    "                    # Get the parent job card element\n",
    "                    parent_card = salary_element.find_element(\n",
    "                        By.XPATH, (\"./ancestor::div[contains(@class, 'jobCard')]\")\n",
    "                    )\n",
    "                    card_link = parent_card.find_element(\n",
    "                        By.CSS_SELECTOR, \"a[data-test='job-link']\"\n",
    "                    )\n",
    "\n",
    "                    # Check if this is the salary for our current job\n",
    "                    if card_link.get_attribute(\"href\").endswith(current_job_href):\n",
    "                        salary_text = salary_element.text.strip()\n",
    "                        if salary_text and salary_text != \"Salary not available\":\n",
    "                            job_data[\"salary\"] = salary_text\n",
    "                            salary_found = True\n",
    "                            print(\n",
    "                                f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Found matching salary: {job_data['salary']}\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                if not salary_found:\n",
    "                    print(\n",
    "                        f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] No salary found, skipping job\"\n",
    "                    )\n",
    "                    return None\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] No job link found, skipping job\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting salary: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # Find and click the job title link\n",
    "        try:\n",
    "            # Check if this job's details are already visible\n",
    "            current_url = driver.current_url\n",
    "            job_href = job_link.get(\"href\")\n",
    "\n",
    "            # Only skip the click if this specific job is already showing\n",
    "            if job_href in current_url:\n",
    "                print(\"This job's details already visible, skipping click\")\n",
    "            else:\n",
    "                job_title_link = WebDriverWait(driver, 1).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            f\"a[data-test='job-link'][href='{job_href}']\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Scroll the element into view\n",
    "                driver.execute_script(\n",
    "                    \"arguments[0].scrollIntoView(true);\", job_title_link\n",
    "                )\n",
    "                time.sleep(uniform(0.05, 0.1))  # Minimal pause after scrolling\n",
    "\n",
    "                # Click using JavaScript\n",
    "                driver.execute_script(\"arguments[0].click();\", job_title_link)\n",
    "\n",
    "                # No need for extra wait here since we'll wait for description element in get_full_description\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking job details: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        # Now extract the job details from the expanded view\n",
    "        job_data[\"job_description\"] = (\n",
    "            get_full_description(driver) or \"Description not available\"\n",
    "        )\n",
    "\n",
    "        # Extract location using XPath\n",
    "        try:\n",
    "            location_element = WebDriverWait(driver, 2).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//*[@id='app-navigation']/div[4]/div[2]/div[2]/div/div[1]/header/div[1]/div\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            job_data[\"location\"] = location_element.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting location: {str(e)}\")\n",
    "            job_data[\"location\"] = \"Location not available\"\n",
    "\n",
    "        # Extract job title by iterating through possible title elements\n",
    "        try:\n",
    "            # Find all title elements in the job cards\n",
    "            title_elements = driver.find_elements(\n",
    "                By.XPATH,\n",
    "                \"//*[@id='left-column']/div[2]/ul/li/div/div/div[1]/div[1]/a[1]\",\n",
    "            )\n",
    "\n",
    "            # Get the current job's href to match with the correct title\n",
    "            current_job_href = job_link.get(\"href\") if job_link else None\n",
    "\n",
    "            if current_job_href:\n",
    "                # Find the matching title element for this job card\n",
    "                for title_element in title_elements:\n",
    "                    # Get the parent job card element\n",
    "                    parent_card = title_element.find_element(\n",
    "                        By.XPATH, \"./ancestor::div[contains(@class, 'jobCard')]\"\n",
    "                    )\n",
    "                    card_link = parent_card.find_element(\n",
    "                        By.CSS_SELECTOR, \"a[data-test='job-link']\"\n",
    "                    )\n",
    "\n",
    "                    # Check if this is the title for our current job\n",
    "                    if card_link.get_attribute(\"href\").endswith(current_job_href):\n",
    "                        job_data[\"job_title\"] = title_element.text.strip()\n",
    "                        print(\n",
    "                            f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Found matching job title: {job_data['job_title']}\"\n",
    "                        )\n",
    "                        break\n",
    "                else:\n",
    "                    job_data[\"job_title\"] = \"Title not available\"\n",
    "            else:\n",
    "                job_data[\"job_title\"] = \"Title not available\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting job title: {str(e)}\")\n",
    "            job_data[\"job_title\"] = \"Title not available\"\n",
    "\n",
    "        # Store the job link for reference\n",
    "        job_link = job_card.find(\"a\", class_=\"JobCard_trackingLink__HMyun\")\n",
    "        if job_link and job_link.get(\"href\"):\n",
    "            href = job_link.get(\"href\")\n",
    "            if href.startswith(\"/\"):\n",
    "                base_url = \"https://www.glassdoor.com\"\n",
    "                if \"glassdoor.sg\" in driver.current_url:\n",
    "                    base_url = \"https://www.glassdoor.sg\"\n",
    "                elif \"glassdoor.co.in\" in driver.current_url:\n",
    "                    base_url = \"https://www.glassdoor.co.in\"\n",
    "                job_data[\"job_link\"] = base_url + href\n",
    "            else:\n",
    "                job_data[\"job_link\"] = href\n",
    "\n",
    "        return job_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting job details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_full_description(driver):\n",
    "    \"\"\"Get the full job description from the expanded job card view.\"\"\"\n",
    "    try:\n",
    "        # First ensure no modal is present\n",
    "        def close_any_modal():\n",
    "            try:\n",
    "                close_button = WebDriverWait(driver, 0.5).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"CloseButton\"))\n",
    "                )\n",
    "                if close_button.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                    print(\n",
    "                        f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Modal detected and closed\"\n",
    "                    )\n",
    "                    time.sleep(uniform(0.05, 0.1))\n",
    "                    return True\n",
    "                return False\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "        # Try to close modal if present\n",
    "        close_any_modal()\n",
    "\n",
    "        # Use the most reliable Show More button selector\n",
    "        try:\n",
    "            show_more_button = WebDriverWait(driver, 0.5).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"button[class*='JobDetails_showMore___']\")\n",
    "                )\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "            print(\n",
    "                f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Clicked 'Show More' button\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"'Show More' button not found\")\n",
    "\n",
    "        # Try to get the description using the specific XPath\n",
    "        try:\n",
    "            description_element = WebDriverWait(driver, 2).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//*[@id='app-navigation']/div[4]/div[2]/div[2]/div/div[1]/section/div[2]/div[1]\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            description = description_element.text.strip()\n",
    "            if description:\n",
    "                print(\n",
    "                    f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Found job description\"\n",
    "                )\n",
    "                return description\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting description using XPath: {str(e)}\")\n",
    "\n",
    "        return \"Description not available\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting full description: {str(e)}\")\n",
    "        return \"Description not available\"\n",
    "\n",
    "\n",
    "def extract_job_id(url):\n",
    "    \"\"\"Extract jobListingId from URL\"\"\"\n",
    "    try:\n",
    "        if \"?\" in url:\n",
    "            params = dict(param.split(\"=\") for param in url.split(\"?\")[1].split(\"&\"))\n",
    "            return params.get(\"jobListingId\")\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_metadata():\n",
    "    \"\"\"Load metadata of previously scraped jobs\"\"\"\n",
    "    if os.path.exists(\"scraping_metadata.json\"):\n",
    "        with open(\"scraping_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Convert the URLs to job IDs\n",
    "            job_ids = {\n",
    "                extract_job_id(url)\n",
    "                for url in data[\"scraped_jobs\"]\n",
    "                if extract_job_id(url)\n",
    "            }\n",
    "            data[\"scraped_jobs\"] = job_ids\n",
    "            return data\n",
    "    return {\"scraped_jobs\": set(), \"last_scrape_date\": None, \"total_jobs_scraped\": 0}\n",
    "\n",
    "\n",
    "def save_metadata(metadata):\n",
    "    \"\"\"Save metadata of scraped jobs\"\"\"\n",
    "    # Convert set to list for JSON serialization\n",
    "    metadata[\"scraped_jobs\"] = list(metadata[\"scraped_jobs\"])\n",
    "    with open(\"scraping_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "\n",
    "def scrape_glassdoor_jobs(query, country, jobs_per_country=50):\n",
    "    \"\"\"Main function to scrape Glassdoor jobs.\n",
    "\n",
    "    Args:\n",
    "        query (str): Job search query\n",
    "        country (str): Country code to search in\n",
    "        jobs_per_country (int): Number of jobs to scrape per country\n",
    "    \"\"\"\n",
    "    if country not in glassdoor_links_map:\n",
    "        raise ValueError(\n",
    "            f\"Country '{country}' not supported. Available countries: {', '.join(glassdoor_links_map.keys())}\"\n",
    "        )\n",
    "\n",
    "    # Load metadata of previously scraped jobs\n",
    "    metadata = load_metadata()\n",
    "    scraped_jobs = set(metadata[\"scraped_jobs\"])  # Convert back to set\n",
    "    print(f\"Found {len(scraped_jobs)} previously scraped jobs\")\n",
    "    print(\"First few scraped job links:\", list(scraped_jobs)[:3])  # Debug print\n",
    "\n",
    "    driver = setup_driver()\n",
    "    jobs = []\n",
    "\n",
    "    # Define CSV headers\n",
    "    fieldnames = [\n",
    "        \"query\",\n",
    "        \"country\",\n",
    "        \"job_description\",\n",
    "        \"location\",\n",
    "        \"salary\",\n",
    "        \"job_title\",\n",
    "        \"job_link\",\n",
    "    ]\n",
    "\n",
    "    # Create or open CSV file\n",
    "    file_exists = os.path.isfile(\"glassdoor.csv\")\n",
    "    with open(\"glassdoor.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "    try:\n",
    "        base_url = glassdoor_links_map[country]\n",
    "        query_formatted = query.replace(\" \", \"-\")\n",
    "        # Calculate the correct query length based on the actual position in the URL\n",
    "        query_len = 14\n",
    "        if country == \"US\":\n",
    "            query_len = len(query_formatted) + 14\n",
    "        elif country == \"SG\":\n",
    "            query_len = len(query_formatted) + 10\n",
    "        elif country == \"IN\":\n",
    "            query_len = len(query_formatted) + 6\n",
    "\n",
    "        url = base_url.format(query=query_formatted, query_len=query_len)\n",
    "\n",
    "        current_job_count = 0\n",
    "        page = 1\n",
    "\n",
    "        while current_job_count < jobs_per_country:\n",
    "            if page > 1:\n",
    "                url = f\"{url}?p={page}\"\n",
    "            print(\n",
    "                f\"\\nProcessing page {page} for {country}. Current jobs: {current_job_count}/{jobs_per_country}\"\n",
    "            )\n",
    "\n",
    "            driver.get(url)\n",
    "            print(\n",
    "                f\"\\n[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Current URL: {url}\"\n",
    "            )\n",
    "            print(f\"Attempting to load job cards on page {page + 1}...\")\n",
    "\n",
    "            try:\n",
    "                # Single wait for job cards to load using a specific selector\n",
    "                WebDriverWait(driver, 1).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.CSS_SELECTOR, \"[data-test='job-link']\")\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Job cards loaded successfully\"\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(\"ERROR: Timeout waiting for job cards to load\")\n",
    "                print(\"Current page source:\")\n",
    "                print(driver.page_source[:500])  # Print first 500 chars of page source\n",
    "                raise\n",
    "\n",
    "            # Parse the page\n",
    "            print(\"Parsing page with BeautifulSoup...\")\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            job_cards = soup.find_all(\"div\", class_=\"jobCard\")\n",
    "            print(\n",
    "                f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Found {len(job_cards)} job cards on the page\"\n",
    "            )\n",
    "\n",
    "            for job_card in job_cards:\n",
    "                # Get job link before full extraction to check if already scraped\n",
    "                job_link = job_card.find(\"a\", class_=\"JobCard_trackingLink__HMyun\")\n",
    "                if job_link and job_link.get(\"href\"):\n",
    "                    href = job_link.get(\"href\")\n",
    "\n",
    "                    if href.startswith(\"/\"):\n",
    "                        base_url = \"https://www.glassdoor.com\"\n",
    "                        if \"glassdoor.sg\" in driver.current_url:\n",
    "                            base_url = \"https://www.glassdoor.sg\"\n",
    "                        elif \"glassdoor.co.in\" in driver.current_url:\n",
    "                            base_url = \"https://www.glassdoor.co.in\"\n",
    "                        full_job_link = base_url + href\n",
    "                    else:\n",
    "                        full_job_link = href\n",
    "\n",
    "                    # Extract jobListingId from the current job link\n",
    "                    current_job_id = extract_job_id(full_job_link)\n",
    "                    if current_job_id and current_job_id in scraped_jobs:\n",
    "                        # Convert set to list for indexing\n",
    "                        scraped_list = list(scraped_jobs)\n",
    "                        index = scraped_list.index(current_job_id)\n",
    "                        print(\n",
    "                            f\"Skipping already scraped job (index {index}): jobListingId={current_job_id}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                job = extract_job_details(soup, driver, job_card)\n",
    "                if job:\n",
    "                    # Add country and search term information\n",
    "                    job[\"country\"] = country\n",
    "                    job[\"query\"] = query\n",
    "\n",
    "                    # Job description is already fetched in extract_job_details\n",
    "                    if not job.get(\"job_description\"):\n",
    "                        job[\"job_description\"] = \"Description not available\"\n",
    "\n",
    "                    # Add job ID to scraped jobs set\n",
    "                    job_id = extract_job_id(job[\"job_link\"])\n",
    "                    if job_id:\n",
    "                        scraped_jobs.add(job_id)\n",
    "\n",
    "                    # Save to CSV\n",
    "                    with open(\n",
    "                        \"glassdoor.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\"\n",
    "                    ) as f:\n",
    "                        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                        writer.writerow(job)\n",
    "\n",
    "                    jobs.append(job)\n",
    "                    current_job_count += 1\n",
    "\n",
    "                    # Check if we've reached the target number of jobs\n",
    "                    if current_job_count >= jobs_per_country:\n",
    "                        print(\n",
    "                            f\"Reached target of {jobs_per_country} jobs for {country}\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "            # Break the loop if we've reached the target\n",
    "            if current_job_count >= jobs_per_country:\n",
    "                break\n",
    "\n",
    "            # After processing all job cards, try to click \"Show More Jobs\" button\n",
    "            try:\n",
    "                # Quick check for modal\n",
    "                try:\n",
    "                    close_button = driver.find_element(By.CLASS_NAME, \"CloseButton\")\n",
    "                    if close_button.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "                except:\n",
    "                    pass  # No modal present, continue\n",
    "\n",
    "                show_more_jobs = WebDriverWait(driver, 3).until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (By.XPATH, '//*[@id=\"left-column\"]/div[2]/div/div/button')\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Scroll the button into view and ensure it's clickable\n",
    "                driver.execute_script(\n",
    "                    \"arguments[0].scrollIntoView({block: 'center'});\", show_more_jobs\n",
    "                )\n",
    "                WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (By.XPATH, '//*[@id=\"left-column\"]/div[2]/div/div/button')\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Click using JavaScript\n",
    "                driver.execute_script(\"arguments[0].click();\", show_more_jobs)\n",
    "                print(\n",
    "                    f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Clicked 'Show More Jobs' button\"\n",
    "                )\n",
    "\n",
    "                # Wait for new job cards to load (wait for count to increase)\n",
    "                old_count = len(driver.find_elements(By.CLASS_NAME, \"jobCard\"))\n",
    "                try:\n",
    "                    WebDriverWait(driver, 0.5).until(\n",
    "                        lambda x: len(x.find_elements(By.CLASS_NAME, \"jobCard\"))\n",
    "                        > old_count\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    print(\"Timeout waiting for new cards, continuing anyway...\")\n",
    "\n",
    "                # Update the soup and job cards with new content\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                job_cards = soup.find_all(\"div\", class_=\"jobCard\")\n",
    "                print(\n",
    "                    f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] Found {len(job_cards)} job cards after loading more\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"No more jobs to load or error clicking 'Show More Jobs' button: {str(e)}\"\n",
    "                )\n",
    "                break  # Exit the loop if we can't load more jobs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {str(e)}\")\n",
    "    finally:\n",
    "        # Update metadata before quitting\n",
    "        metadata[\"scraped_jobs\"] = list(scraped_jobs)\n",
    "        metadata[\"last_scrape_date\"] = datetime.now().isoformat()\n",
    "        metadata[\"total_jobs_scraped\"] = len(scraped_jobs)\n",
    "        save_metadata(metadata)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    return jobs\n",
    "\n",
    "\n",
    "# Common job search terms\n",
    "COMMON_SEARCH_TERMS = [\n",
    "    # \"software engineer\",\n",
    "    # \"data scientist\",\n",
    "    # \"product manager\",\n",
    "    # \"data analyst\",\n",
    "    # \"software developer\",\n",
    "    # \"project manager\",\n",
    "    # \"business analyst\",\n",
    "    # \"full stack developer\",\n",
    "    # \"data engineer\",\n",
    "    # \"frontend developer\",\n",
    "    # \"backend developer\",\n",
    "    # \"devops engineer\",\n",
    "    # \"machine learning engineer\",\n",
    "    # \"systems engineer\",\n",
    "    # \"qa engineer\",\n",
    "    # \"cloud engineer\",\n",
    "    # \"java developer\",\n",
    "    # \"python developer\",\n",
    "    # \"web developer\",\n",
    "    # \"solutions architect\",\n",
    "    # \"it manager\",\n",
    "    # \"network engineer\",\n",
    "    # \"security engineer\",\n",
    "    # \"database administrator\",\n",
    "    # \"ui ux designer\",\n",
    "    # \"scrum master\",\n",
    "    # \"android developer\",\n",
    "    # \"ios developer\",\n",
    "    # \"site reliability engineer\",\n",
    "    # \"technical lead\",\n",
    "    # \"automation engineer\",\n",
    "    # \"research scientist\",\n",
    "    # \"ai engineer\",\n",
    "    # \"blockchain developer\",\n",
    "    # \"cloud architect\",\n",
    "    # \"cybersecurity analyst\",\n",
    "    # \"data architect\",\n",
    "    # \"embedded engineer\",\n",
    "    \"full stack engineer\",\n",
    "    \"infrastructure engineer\",\n",
    "    \"javascript developer\",\n",
    "    \"mobile developer\",\n",
    "    \"network administrator\",\n",
    "    \"product owner\",\n",
    "    \"quality assurance\",\n",
    "    \"ruby developer\",\n",
    "    \"security analyst\",\n",
    "    \"software architect\",\n",
    "    \"systems administrator\",\n",
    "    \"technical architect\",\n",
    "    \"unity developer\",\n",
    "    \"accountant\",\n",
    "    \"financial analyst\",\n",
    "    \"auditor\",\n",
    "    \"financial manager\",\n",
    "    \"actuary\",\n",
    "    \"marketing manager\",\n",
    "    \"marketing specialist\",\n",
    "    \"sales manager\",\n",
    "    \"sales representative\",\n",
    "    \"digital marketing specialist\",\n",
    "    \"graphic designer\",\n",
    "    \"copywriter\",\n",
    "    \"content writer\",\n",
    "    \"public relations specialist\",\n",
    "    \"social media manager\",\n",
    "    \"human resources manager\",\n",
    "    \"hr specialist\",\n",
    "    \"recruiter\",\n",
    "    \"training manager\",\n",
    "    \"payroll specialist\",\n",
    "    \"teacher\",\n",
    "    \"professor\",\n",
    "    \"instructional designer\",\n",
    "    \"principal\",\n",
    "    \"school counselor\",\n",
    "    \"nurse\",\n",
    "    \"physician\",\n",
    "    \"pharmacist\",\n",
    "    \"medical assistant\",\n",
    "    \"physical therapist\",\n",
    "    \"registered nurse\",\n",
    "    \"medical doctor\",\n",
    "    \"therapist\",\n",
    "    \"project coordinator\",\n",
    "    \"operations manager\",\n",
    "    \"supply chain manager\",\n",
    "    \"logistics coordinator\",\n",
    "    \"purchasing manager\",\n",
    "    \"restaurant manager\",\n",
    "    \"chef\",\n",
    "    \"bartender\",\n",
    "    \"waiter/waitress\",\n",
    "    \"event planner\",\n",
    "    \"hotel manager\",\n",
    "    \"civil engineer\",\n",
    "    \"electrical engineer\",\n",
    "    \"mechanical engineer\",\n",
    "    \"chemical engineer\",\n",
    "    \"environmental engineer\",\n",
    "    \"architect\",\n",
    "    \"urban planner\",\n",
    "    \"construction manager\",\n",
    "    \"biomedical engineer\",\n",
    "    \"manufacturing engineer\",\n",
    "    \"legal assistant\",\n",
    "    \"paralegal\",\n",
    "    \"lawyer\",\n",
    "    \"attorney\",\n",
    "    \"legal secretary\",\n",
    "    \"data entry clerk\",\n",
    "    \"office manager\",\n",
    "    \"administrative assistant\",\n",
    "    \"customer service representative\",\n",
    "    \"executive assistant\",\n",
    "    \"receptionist\",\n",
    "    \"business development manager\",\n",
    "    \"management consultant\",\n",
    "    \"market research analyst\",\n",
    "    \"statistician\",\n",
    "    \"economist\",\n",
    "    \"ux researcher\",\n",
    "    \"technical writer\",\n",
    "    \"scientific writer\",\n",
    "    \"librarian\",\n",
    "    \"journalist\",\n",
    "    \"editor\",\n",
    "    \"translator\",\n",
    "    \"interpreter\",\n",
    "    \"pharmacovigilance specialist\",\n",
    "    \"clinical research associate\",\n",
    "    \"biostatistician\",\n",
    "    \"regulatory affairs specialist\",\n",
    "    \"lab technician\",\n",
    "    \"research associate\",\n",
    "    \"geneticist\",\n",
    "    \"zoologist\",\n",
    "    \"geologist\",\n",
    "    \"astronomer\",\n",
    "    \"mathematician\",\n",
    "    \"actuarial analyst\",\n",
    "    \"investment banker\",\n",
    "    \"portfolio manager\",\n",
    "    \"loan officer\",\n",
    "    \"risk manager\",\n",
    "    \"compliance officer\",\n",
    "    \"estate agent\",\n",
    "    \"insurance agent\",\n",
    "    \"real estate agent\",\n",
    "    \"social worker\",\n",
    "    \"psychologist\",\n",
    "    \"counselor\",\n",
    "]\n",
    "\n",
    "\n",
    "# Glassdoor country-specific URLs\n",
    "glassdoor_links_map = {\n",
    "    \"US\": \"https://www.glassdoor.com/Job/united-states-{query}-jobs-SRCH_IL.0,13_IN1_KO14,{query_len}.htm\",\n",
    "    \"SG\": \"https://www.glassdoor.sg/Job/singapore-{query}-jobs-SRCH_IL.0,9_IN217_KO10,{query_len}.htm\",\n",
    "    \"IN\": \"https://www.glassdoor.co.in/Job/india-{query}-jobs-SRCH_IL.0,5_IN115_KO6,{query_len}.htm?includeNoSalaryJobs=true\",\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    JOBS_PER_COUNTRY = 30\n",
    "\n",
    "    # Create a log file for the entire scraping session\n",
    "    session_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(\n",
    "        f\"scraping_session_{session_timestamp}.log\", \"w\", encoding=\"utf-8\"\n",
    "    ) as log_file:\n",
    "        # Iterate through each search term\n",
    "        for search_term in COMMON_SEARCH_TERMS:\n",
    "            log_file.write(\n",
    "                f\"\\n{'='*50}\\nProcessing search term: {search_term}\\n{'='*50}\\n\"\n",
    "            )\n",
    "            print(f\"\\n{'='*50}\\nProcessing search term: {search_term}\\n{'='*50}\")\n",
    "\n",
    "            # Iterate through each country\n",
    "            for country in glassdoor_links_map.keys():\n",
    "                message = (\n",
    "                    f\"\\nScraping Glassdoor jobs for {country} - Search: {search_term}\"\n",
    "                )\n",
    "                print(message)\n",
    "                log_file.write(message + \"\\n\")\n",
    "\n",
    "                try:\n",
    "                    jobs = scrape_glassdoor_jobs(\n",
    "                        search_term, country, jobs_per_country=JOBS_PER_COUNTRY\n",
    "                    )\n",
    "\n",
    "                    message = f\"Successfully scraped {len(jobs)} jobs from {country} for {search_term}\"\n",
    "                    print(message)\n",
    "                    log_file.write(message + \"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    error_message = (\n",
    "                        f\"Error scraping {country} for {search_term}: {str(e)}\"\n",
    "                    )\n",
    "                    print(error_message)\n",
    "                    log_file.write(error_message + \"\\n\")\n",
    "\n",
    "                # Add a minimal delay between countries\n",
    "                time.sleep(uniform(0.1, 0.5))\n",
    "\n",
    "            # Add a minimal delay between search terms\n",
    "            time.sleep(uniform(0.1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jukit_cell_id": "0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading glassdoor.csv...\n",
      "\n",
      "=== BASIC INFORMATION ===\n",
      "Total number of rows: 3513\n",
      "Total number of columns: 7\n",
      "\n",
      "Columns: ['query', 'country', 'job_description', 'location', 'salary', 'job_title', 'job_link']\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "           Missing Count  Missing Percentage\n",
      "job_title              4                0.11\n",
      "\n",
      "=== DUPLICATES ===\n",
      "Total duplicate rows: 0\n",
      "Rows with duplicate job links: 0\n",
      "\n",
      "=== VALUE DISTRIBUTIONS ===\n",
      "\n",
      "Country distribution:\n",
      "country\n",
      "US    1229\n",
      "IN    1178\n",
      "SG    1106\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 job titles:\n",
      "job_title\n",
      "Data Scientist          54\n",
      "DevOps Engineer         47\n",
      "Data Analyst            45\n",
      "Software Engineer       45\n",
      "Data Engineer           40\n",
      "Scrum Master            38\n",
      "Software Developer      37\n",
      "Network Engineer        37\n",
      "Full Stack Developer    31\n",
      "Business Analyst        29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== POTENTIAL DATA QUALITY ISSUES ===\n",
      "\n",
      "Jobs with very short descriptions (<100 chars): 12\n",
      "Jobs with potentially invalid salaries: 69\n",
      "\n",
      "Unique locations found:\n",
      "location\n",
      "Singapore            832\n",
      "Bengaluru            231\n",
      "Remote               178\n",
      "Pune                  82\n",
      "Hyderābād             72\n",
      "United States         56\n",
      "Gurgaon               49\n",
      "Mumbai                33\n",
      "Queenstown Estate     31\n",
      "Chennai               29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved 85 problematic entries to 'problematic_entries.csv'\n",
      "\n",
      "=== TOKEN USAGE ANALYSIS ===\n",
      "\n",
      "Template tokens (per request): 167\n",
      "\n",
      "=== TOTAL TOKEN USAGE AND COST ===\n",
      "Total Input Tokens: 2654369\n",
      "Total Output Tokens: 333735\n",
      "Input Cost: $39.8155\n",
      "Output Cost: $25.0301\n",
      "Total Cost: $64.8457\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "import tiktoken\n",
    "import json\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def analyze_glassdoor_data():\n",
    "    # Read the CSV file\n",
    "    print(\"\\nReading glassdoor.csv...\")\n",
    "    df = pd.read_csv(\"glassdoor.csv\")\n",
    "\n",
    "    # Basic information about the dataset\n",
    "    print(\"\\n=== BASIC INFORMATION ===\")\n",
    "    print(f\"Total number of rows: {len(df)}\")\n",
    "    print(f\"Total number of columns: {len(df.columns)}\")\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\n=== MISSING VALUES ===\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentages = (missing_values / len(df)) * 100\n",
    "    missing_info = pd.DataFrame(\n",
    "        {\n",
    "            \"Missing Count\": missing_values,\n",
    "            \"Missing Percentage\": missing_percentages.round(2),\n",
    "        }\n",
    "    )\n",
    "    print(missing_info[missing_info[\"Missing Count\"] > 0])\n",
    "\n",
    "    # Check for duplicates\n",
    "    print(\"\\n=== DUPLICATES ===\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Total duplicate rows: {duplicates}\")\n",
    "\n",
    "    # Check for duplicate job links (same job posted multiple times)\n",
    "    duplicate_links = df[df.duplicated(subset=[\"job_link\"], keep=False)]\n",
    "    print(f\"Rows with duplicate job links: {len(duplicate_links)}\")\n",
    "\n",
    "    # Value distributions\n",
    "    print(\"\\n=== VALUE DISTRIBUTIONS ===\")\n",
    "    print(\"\\nCountry distribution:\")\n",
    "    print(df[\"country\"].value_counts())\n",
    "\n",
    "    print(\"\\nTop 10 job titles:\")\n",
    "    print(df[\"job_title\"].value_counts().head(10))\n",
    "\n",
    "    # Check for potential data quality issues\n",
    "    print(\"\\n=== POTENTIAL DATA QUALITY ISSUES ===\")\n",
    "\n",
    "    # Check for very short or empty descriptions\n",
    "    short_desc = df[df[\"job_description\"].str.len() < 100]\n",
    "    print(f\"\\nJobs with very short descriptions (<100 chars): {len(short_desc)}\")\n",
    "\n",
    "    # Check for invalid salaries (if they don't contain numbers)\n",
    "    invalid_salaries = df[~df[\"salary\"].str.contains(r\"\\d\", na=True)]\n",
    "    print(f\"Jobs with potentially invalid salaries: {len(invalid_salaries)}\")\n",
    "\n",
    "    # Check for unusual locations\n",
    "    print(\"\\nUnique locations found:\")\n",
    "    print(df[\"location\"].value_counts().head(10))\n",
    "\n",
    "    # Save problematic entries to a separate CSV for review\n",
    "    problematic = df[\n",
    "        (df.isnull().any(axis=1))  # Any missing values\n",
    "        | (df.duplicated())  # Duplicates\n",
    "        | (df[\"job_description\"].str.len() < 100)  # Short descriptions\n",
    "        | (~df[\"salary\"].str.contains(r\"\\d\", na=True))  # Invalid salaries\n",
    "    ]\n",
    "\n",
    "    if len(problematic) > 0:\n",
    "        problematic.to_csv(\"problematic_entries.csv\", index=False)\n",
    "        print(\n",
    "            f\"\\nSaved {len(problematic)} problematic entries to 'problematic_entries.csv'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens using tiktoken\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # Claude's encoding\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def calculate_claude_cost(input_tokens: int, output_tokens: int) -> Dict[str, float]:\n",
    "    \"\"\"Calculate Claude API cost based on token usage\"\"\"\n",
    "    input_cost_per_1k = 0.015\n",
    "    output_cost_per_1k = 0.075\n",
    "\n",
    "    input_cost = (input_tokens / 1000) * input_cost_per_1k\n",
    "    output_cost = (output_tokens / 1000) * output_cost_per_1k\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"input_cost\": round(input_cost, 4),\n",
    "        \"output_cost\": round(output_cost, 4),\n",
    "        \"total_cost\": round(total_cost, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_token_usage():\n",
    "    \"\"\"Analyze token usage and cost for EDA processing\"\"\"\n",
    "    print(\"\\n=== TOKEN USAGE ANALYSIS ===\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(\"glassdoor.csv\")\n",
    "\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    # Template tokens (counted once)\n",
    "    template = \"\"\"Analyze this job posting and extract the following features in JSON format:\n",
    "        - soft_skills: List of soft skills mentioned (communication, leadership, etc)\n",
    "        - hard_skills: List of technical skills, tools, languages required\n",
    "        - location_flexibility: One of [remote, hybrid, onsite, unspecified]\n",
    "        - contract_type: One of [full-time, part-time, contract, internship, unspecified] \n",
    "        - education_level: Minimum required education level [high_school, bachelors, masters, phd, unspecified]\n",
    "        - field_of_study: Required field of study or major\n",
    "        - min_years_experience: Minimum years of experience required (numeric, -1 if unspecified)\n",
    "        - salary_range: Extract salary range if available [min, max, currency, period(yearly/monthly/hourly)]\"\"\"\n",
    "\n",
    "    template_tokens = count_tokens(template)\n",
    "    print(f\"\\nTemplate tokens (per request): {template_tokens}\")\n",
    "\n",
    "    # Analyze each job\n",
    "    for _, row in df.iterrows():\n",
    "        prompt_text = f\"\"\"\n",
    "        Job Title: {row['job_title']}\n",
    "        Location: {row['location']}\n",
    "        Salary: {row['salary']}\n",
    "        Description: {row['job_description']}\n",
    "        \"\"\"\n",
    "\n",
    "        input_tokens = template_tokens + count_tokens(prompt_text)\n",
    "        total_input_tokens += input_tokens\n",
    "\n",
    "        # Estimate output tokens based on typical JSON response\n",
    "        sample_output = {\n",
    "            \"soft_skills\": [\"communication\", \"teamwork\"],\n",
    "            \"hard_skills\": [\"python\", \"sql\"],\n",
    "            \"location_flexibility\": \"remote\",\n",
    "            \"contract_type\": \"full-time\",\n",
    "            \"education_level\": \"bachelors\",\n",
    "            \"field_of_study\": \"computer science\",\n",
    "            \"min_years_experience\": 3,\n",
    "            \"salary_range\": {\n",
    "                \"min\": 80000,\n",
    "                \"max\": 120000,\n",
    "                \"currency\": \"USD\",\n",
    "                \"period\": \"yearly\",\n",
    "            },\n",
    "        }\n",
    "        output_tokens = count_tokens(json.dumps(sample_output))\n",
    "        total_output_tokens += output_tokens\n",
    "\n",
    "    # Calculate total cost\n",
    "    cost_analysis = calculate_claude_cost(total_input_tokens, total_output_tokens)\n",
    "\n",
    "    print(\"\\n=== TOTAL TOKEN USAGE AND COST ===\")\n",
    "    print(f\"Total Input Tokens: {cost_analysis['input_tokens']}\")\n",
    "    print(f\"Total Output Tokens: {cost_analysis['output_tokens']}\")\n",
    "    print(f\"Input Cost: ${cost_analysis['input_cost']}\")\n",
    "    print(f\"Output Cost: ${cost_analysis['output_cost']}\")\n",
    "    print(f\"Total Cost: ${cost_analysis['total_cost']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_glassdoor_data()\n",
    "    analyze_token_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
